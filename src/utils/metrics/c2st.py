from typing import Optional, Union

import numpy as np
import torch
import torch.nn as nn
from torch import Tensor
# noinspection PyProtectedMember
from torch.optim import Adam
from torch.utils.data import Dataset, random_split, DataLoader, TensorDataset
from tqdm.autonotebook import tqdm

from modules.discriminators.dcgan import DCGanDiscriminator
from modules.ifaces import IGanGModule
from utils.ifaces import Freezable


class C2ST(nn.Module):
    """
    C2ST Class:
    This class is used to compute the Classifier 2-Sample Test (C2ST) metric between real and generated images as
    proposed in the paper "Revisiting Classifier Two-Sample Tests".
    Inspired by: https://github.com/mkirchler/deep-2-sample-test/blob/master/deeptest/c2st.py
    """

    def __init__(self, gan_instance: IGanGModule, device: torch.device or str = 'cpu',
                 train_epochs: int = 10, use_sklearn: bool = True, **kwargs):
        """
        C2ST class constructor.
        :param (str) device: the device type on which to run the Inception model (defaults to 'cpu')
        :param (int) train_epochs: number of epochs to train the discriminator for
        :param (bool) use_sklearn: set to True to train/eval the classifier using Scikit-Learn builtin classes
        """
        super(C2ST, self).__init__()
        self.epochs = train_epochs
        self.use_sklearn = use_sklearn
        self.n_bootstraps = 10
        self.real_vs_real = False

        self.gan = gan_instance
        self.batch_size = 64
        self.device = device

    # noinspection PyUnusedLocal
    def forward(self, dataset: Dataset, gen: nn.Module, target_index: Optional[int] = None,
                condition_indices: Optional[Union[int, tuple]] = None, z_dim: Optional[int] = None,
                show_progress: bool = True, use_fid_embeddings: bool = False, **kwargs) -> Tensor:
        """
        Compute the C2ST score between random $self.n_samples$ images from the given dataset and same number of images
        generated by the given generator network.
        :param dataset: a torch.utils.data.Dataset object to access real images. Attention: no transforms should be
                        applied when __getitem__ is called since the transforms are different on Inception v3
        :param gen: the Generator network
        :param target_index: index of target (real) output from the arguments that returns dataset::__getitem__() method
        :param condition_indices: indices of images that will be passed to the Generator in order to generate fake
                                  images (for image-to-image translation tasks). If set to None, the generator is fed
                                  with random noise.
        :param z_dim: if $condition_indices$ is None, then this is necessary to produce random noise to feed into the
                      DCGAN-like generator
        :param (bool) show_progress: set to True to display progress using `tqdm` lib
        :param (bool) use_fid_embeddings: set to True to avoid re-computing ImageNET embeddings and use the ones
                                          computed during calculation of the FID metric
        :return: the C2ST value as a torch.Tensor object
        """
        # Freeze Generator
        assert isinstance(gen, Freezable), 'Generator should implement utils.ifaces.Freezable'
        with gen.frozen():
            # Generate fake images
            if hasattr(self.gan, 'fixed_noise_lambda'):
                noise = self.gan.fixed_noise_lambda()
            else:
                noise = gen.get_random_z(batch_size=self.batch_size)
            batch_size = noise.shape[0]
            noise = noise.to(self.device)
            gen.eval()
            fake_images = gen(noise)

            # Create the dataloader for real samples
            train_set, test_set, _ = random_split(dataset, [batch_size, batch_size, len(dataset) - 2 * batch_size])
            real_dl = DataLoader(dataset=train_set, batch_size=8, shuffle=True)
            real_test_dl = DataLoader(dataset=test_set, batch_size=8, shuffle=True)
            if self.device == 'cuda:0' and torch.cuda.is_available():
                torch.cuda.empty_cache()

            # Create the dataloader for fake images
            fake_ds = TensorDataset(fake_images)
            fake_dl = DataLoader(fake_ds, batch_size=8, shuffle=True)

            # Create the Discriminator instance
            disc_conf = self.gan._configuration['disc']
            if 'c_in' not in disc_conf.keys():
                disc_conf['c_in'] = 2
            disc = DCGanDiscriminator(**disc_conf)
            disc = disc.to(self.device).train()
            disc_opt = Adam(disc.parameters(), lr=1e-3, betas=(0.9, 0.999))

            # Train the discriminator for 100 epochs
            disc_loss_avg = []
            pbar = tqdm(range(self.epochs), desc=f'C2ST (loss=-.---)')
            for e in pbar:
                for real, (fake,) in zip(real_dl, fake_dl):
                    disc_opt.zero_grad()  # Zero out discriminator gradient (before backprop)
                    disc_loss = disc.get_loss_both(real=real.to(self.device).detach(), fake=fake.to(self.device).detach()).mean()
                    disc_loss.backward()  # Update discriminator gradients
                    disc_opt.step()  # Update discriminator weights
                    disc_loss_avg.append(disc_loss.item())
                pbar.set_description(f'C2ST (loss={np.mean(disc_loss_avg):.3f})')

            # Evaluate on test set
            disc.eval()
            score = 0.0
            num_batches = 0
            for data_real in real_test_dl:
                data_fake = gen(gen.get_random_z(batch_size=data_real.shape[0], device=self.device))
                score_var = disc.get_loss_both(data_real.to(self.device).detach(), data_fake.to(self.device).detach()).mean()
                score += score_var.item()
                num_batches += 1

            # Return
            gen.train()
            print(f'result = {(score.data / num_batches).item():.3f}')
            return score.data / num_batches

        # # Create dataset/dataloader
        # if self.real_vs_real:
        #     train_labels = torch.cat((torch.zeros(len(real_embeddings) // 2), torch.ones(len(real_embeddings) // 2)))
        #     train_set = torch.utils.data.TensorDataset(real_embeddings, train_labels)
        # else:
        #     train_labels = torch.cat((torch.zeros(len(real_embeddings)), torch.ones(len(fake_embeddings))))
        #     train_set = torch.utils.data.TensorDataset(torch.cat((real_embeddings, fake_embeddings)), train_labels)
        #
        # len_ = len(train_set)
        # train_dataset, test_dataset = random_split(train_set,
        #                                            [round(len_ * 0.75), round(len_ * 0.25)])  # 75-25 split
        # train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
        # test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)
        # # Create classifier and optimizer
        # model = nn.Sequential(nn.Linear(real_embeddings.shape[1], 1), nn.Sigmoid())
        # optim = torch.optim.Adam(model.parameters())
        # criterion = nn.BCELoss()
        # model.train()
        # # Train model on training set
        # for ep in range(self.epochs):
        #     for i, (data, target) in enumerate(train_loader):
        #         optim.zero_grad()
        #         p = model(data)
        #         loss = criterion(p.flatten(), target.flatten())
        #         loss.backward()
        #         optim.step()
        # # Compute test set accuracy
        # model.eval()
        # with torch.no_grad():
        #     accuracy = 0.0
        #     n = 0
        #     for i, (data, target) in enumerate(test_loader):
        #         p = model(data).numpy()
        #         accuracy += target.shape[0] * accuracy_score(target, p >= 0.5)
        #         n += target.shape[0]
        # accuracy /= n
        # p_value = 1.0 - stats.norm.cdf(accuracy, loc=0.5, scale=np.sqrt(0.25 / real_embeddings.shape[0]))
        # return p_value
