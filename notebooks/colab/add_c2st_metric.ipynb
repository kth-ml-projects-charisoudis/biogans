{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "train_6class_ind_sep.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1) Mount drive, unzip data, clone repo, install packages"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Delm8c4mHpX_"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "UmWJHQh7r6Kb"
   },
   "source": [
    "## 1.1) Mount Drive and define paths\n",
    "Run provided colab code to mount Google Drive. Then define dataset paths relative to mount point."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "1j__4kcZr6Kc"
   },
   "source": [
    "# noinspection PyUnresolvedReferences,PyPackageRequirements\n",
    "from google.colab import drive\n",
    "\n",
    "mount_root_abs = '/content/drive'\n",
    "drive.mount(mount_root_abs, force_remount=True)\n",
    "drive_root = f'{mount_root_abs}/MyDrive/ProjectGStorage'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2) Unzip Img directory in Colab\n",
    "By unzipping the `lin-48x80.zip` in Colab before running our model we gain significant disk reading speedups.\n",
    "So, the first step is to unzip images directory, and then save the image directory before proceeding."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "opylFk3cHpYC"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "c7bh7lsyr6Kd"
   },
   "source": [
    "import os\n",
    "\n",
    "# Check if LIN Dataset is present / Download Dataset\n",
    "df_root_drive = f'{drive_root}/Datasets/LIN_48x80'\n",
    "if not os.path.exists(f'{df_root_drive}/lin-48x80.zip'):\n",
    "    !pip install kaggle --upgrade\n",
    "    os.environ['KAGGLE_CONFIG_DIR'] = drive_root\n",
    "    !mkdir -p $df_root_drive\n",
    "    !kaggle datasets download \"achariso/lin-48x80\" -p \"$df_root_drive\"\n",
    "\n",
    "# Unzip\n",
    "if not os.path.exists(f\"/content/data/LIN_48x80/LIN_Normalized_WT_size-48-80_train\"):\n",
    "    !pip install unzip\n",
    "    !mkdir -p \"/content/data/LIN_48x80\"\n",
    "    !cp -f \"$df_root_drive/lin-48x80.zip\" \"/content/data/LIN_48x80\"\n",
    "    !unzip -qq \"/content/data/LIN_48x80/lin-48x80.zip\" -d \"/content/data/LIN_48x80\"\n",
    "    if os.path.exists(f'/content/data/LIN_48x80/LIN_Normalized_WT_size-48-80_train'):\n",
    "        # Create symbolic links\n",
    "        !ln -s \"/content/data/LIN_48x80/LIN_Normalized_WT_size-48-80_train\" \"$df_root_drive/LIN_Normalized_WT_size-48-80_train\"\n",
    "        !ln -s \"/content/data/LIN_48x80/LIN_Normalized_WT_size-48-80_test\" \"$df_root_drive/LIN_Normalized_WT_size-48-80_test\"\n",
    "    else:\n",
    "        print(f'Error: Dataset not found at \"/content/data/LIN_48x80\"')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "rg1ABU-Dr6Ke"
   },
   "source": [
    "## 1.3) Clone GitHub repo\n",
    "Clone achariso/gans-thesis repo into /content/code\n",
    " using git clone.\n",
    " For more info see: https://medium.com/@purba0101/how-to-clone-private-github-repo-in-google-colab-using-ssh-77384cfef18f"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "JleSSDBur6Ke"
   },
   "source": [
    "repo_root = '/content/code/biogans'\n",
    "!rm -rf \"$repo_root\"\n",
    "if not os.path.exists(repo_root) and not os.path.exists(f'{repo_root}/requirements.txt'):\n",
    "    # Check that ssh keys exist\n",
    "    assert os.path.exists(f'{drive_root}/GitHub Keys')\n",
    "    id_rsa_abs_drive = f'{drive_root}/GitHub Keys/id_rsa'\n",
    "    id_rsa_pub_abs_drive = f'{id_rsa_abs_drive}.pub'\n",
    "    assert os.path.exists(id_rsa_abs_drive)\n",
    "    assert os.path.exists(id_rsa_pub_abs_drive)\n",
    "    # On first run: Add ssh key in repo\n",
    "    if not os.path.exists('/root/.ssh'):\n",
    "        # Transfer config file\n",
    "        ssh_config_abs_drive = f'{drive_root}/GitHub Keys/config'\n",
    "        assert os.path.exists(ssh_config_abs_drive)\n",
    "        !mkdir -p ~ /.ssh\n",
    "        !cp -f \"$ssh_config_abs_drive\" ~ /.ssh /\n",
    "        # # Add github.com to known hosts\n",
    "        !ssh-keyscan -t rsa github.com >> ~ /.ssh / known_hosts\n",
    "        # Test: !ssh -T git@github.com\n",
    "\n",
    "    # Remove any previous attempts\n",
    "    !rm -rf \"$repo_root\"\n",
    "    !mkdir -p \"$repo_root\"\n",
    "    # Clone repo\n",
    "    !git clone git @ github.com:kth-ml-course-projects / biogans.git \"$repo_root\"\n",
    "    src_root = f'{repo_root}/src'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ZI4gCyMdr6Kf"
   },
   "source": [
    "## 1.4) Install pip packages\n",
    "All required files are stored in a requirements.txt files at the repository's root.\n",
    "Use `pip install -r requirements.txt` from inside the dir to install required packages."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "-v3aIKDXr6Kf"
   },
   "source": [
    "% cd \"$repo_root\"\n",
    "!pip install -r requirements.txt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "oAwLF973r6Kg"
   },
   "source": [
    "# import os\n",
    "# os.kill(os.getpid(), 9)\n",
    "\n",
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tKxMM8dTr6Kh"
   },
   "source": [
    "## 1.5) Add code/, */src/ to path\n",
    "This is necessary in order to be able to run the modules."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "SatLo0V5r6Kh"
   },
   "source": [
    "content_root_abs = f'{repo_root}'\n",
    "src_root_abs = f'{repo_root}/src'\n",
    "% env PYTHONPATH=\"/env/python:$content_root_abs:$src_root_abs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tDfCUlQsr6Kh"
   },
   "source": [
    "# 2) Add C2ST Metric to Existing Ones\n",
    "The method has already implemented in `grdive/__init__`. Just a call to it is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "T3i1qXear6Ki"
   },
   "source": [
    "### Actual Run\n",
    "Eventually, run the code!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "MzfIhC7sr6Ki"
   },
   "source": [
    "chkpt_step = None  # supported: 'latest', <int>, None\n",
    "log_level = 'debug'  # supported: 'debug', 'info', 'warning', 'error', 'critical', 'fatal'\n",
    "device = 'cuda'  # supported: 'cpu', 'cuda', 'cuda:<GPU_INDEX>'\n",
    "gdrive_which = 'personal'  # supported: 'personal', 'auth'\n",
    "\n",
    "# Running with -i enables us to get variables defined inside the script (the script runs inline)\n",
    "% run -i src / train_setup.py --log_level $log_level --chkpt_step $chkpt_step --seed 42 --device $device --gdrive_which $gdrive_which -use_refresh_token\n",
    "% cd src /"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "fo8tX5Omr6Ki"
   },
   "source": [
    "### BioGAN Training\n",
    "The code that follows defines the dataloaders/evaluators/models and the main training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "0xSTSSqOr6Kj"
   },
   "source": [
    "import torch\n",
    "from IPython.core.display import display\n",
    "from torch import Tensor\n",
    "from torch.nn import DataParallel\n",
    "# noinspection PyProtectedMember\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets.lin import LINDataloader6Class\n",
    "from modules.biogan_ind import BioGanInd6Class\n",
    "from utils.metrics import GanEvaluator6Class\n",
    "\n",
    "###################################\n",
    "###  Hyper-parameters settings  ###\n",
    "###################################\n",
    "#   - training\n",
    "n_epochs = 540\n",
    "\n",
    "batch_size = 32 if not run_locally else 2\n",
    "train_test_splits = [90, 10]  # for a 90% training - 10% evaluation set split\n",
    "#   - evaluation\n",
    "metrics_n_samples = 1000 if not run_locally else 2\n",
    "metrics_batch_size = 32 if not run_locally else 1\n",
    "f1_k = 3 if not run_locally else 1\n",
    "#   - visualizations / checkpoints steps\n",
    "display_step = 150\n",
    "checkpoint_step = 6 * display_step\n",
    "metrics_step = 3 * checkpoint_step  # evaluate model every 3 checkpoints\n",
    "\n",
    "###################################\n",
    "###   Dataset Initialization    ###\n",
    "###################################\n",
    "#   - the dataloader used to access the training dataset of cross-scale/pose image pairs at every epoch\n",
    "#     > len(dataloader) = <number of batches>\n",
    "#     > len(dataloader.dataset) = <number of total dataset items>\n",
    "# FIX: Add subfolders to GoogleDrive\n",
    "datasets_groot.subfolder_by_name('LIN_48x80').subfolder_by_name_or_create('LIN_Normalized_WT_size-48-80_train')\n",
    "datasets_groot.subfolder_by_name('LIN_48x80').subfolder_by_name_or_create('LIN_Normalized_WT_size-48-80_test')\n",
    "dataloader = LINDataloader6Class(dataset_fs_folder_or_root=datasets_groot, train_not_test=True,\n",
    "                                 batch_size=batch_size, pin_memory=not run_locally, shuffle=True)\n",
    "dataset = dataloader.dataset\n",
    "dataset.logger.debug('Transforms: ' + repr(dataset.transforms))\n",
    "#   - apply rudimentary tests\n",
    "assert issubclass(dataloader.__class__, DataLoader)\n",
    "assert len(dataloader) == len(dataset) // batch_size + (1 if len(dataset) % batch_size else 0)\n",
    "_x = next(iter(dataloader))\n",
    "assert tuple(_x.shape) == (6, batch_size, 2, 48, 80)\n",
    "\n",
    "###################################\n",
    "###    Models Initialization    ###\n",
    "###################################\n",
    "#   - initialize evaluator instance (used to run GAN evaluation metrics: FID, IS, PRECISION, RECALL, F1 and SSIM)\n",
    "evaluator = GanEvaluator6Class(\n",
    "    model_fs_folder_or_root=models_groot, gen_dataset=dataset, device=exec_device, z_dim=-1,\n",
    "    n_samples=metrics_n_samples, batch_size=metrics_batch_size, f1_k=f1_k, ssim_c_img=2)\n",
    "#   - initialize model\n",
    "biogan_config = 'wgan-gp-independent-sep'  # or 'gan-independent-sep', 'wgan-gp-independent-sep', 'default'\n",
    "chkpt_step = args.chkpt_step\n",
    "try:\n",
    "    if chkpt_step == 'latest':\n",
    "        _chkpt_step = chkpt_step\n",
    "    elif isinstance(chkpt_step, str) and chkpt_step.isdigit():\n",
    "        _chkpt_step = int(chkpt_step)\n",
    "    else:\n",
    "        _chkpt_step = None\n",
    "except NameError:\n",
    "    _chkpt_step = None\n",
    "biogan = BioGanInd6Class(model_fs_folder_or_root=models_groot, config_id=biogan_config, dataset_len=len(dataset),\n",
    "                         chkpt_epoch=_chkpt_step, evaluator=evaluator, device=exec_device, log_level=log_level,\n",
    "                         gen_transforms=dataloader.transforms)\n",
    "biogan.logger.debug(f'Using device: {str(exec_device)}')\n",
    "biogan.logger.debug(f'Model initialized. Number of params = {biogan.nparams_hr}')\n",
    "# # FIX: Warmup counters before first batch\n",
    "# if biogan.step is None:\n",
    "#     biogan.gforward(batch_size=batch_size)\n",
    "#     biogan.logger.debug(f'Model warmed-up (internal counters).')\n",
    "# #   - setup multi-GPU training\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     biogan.gen = DataParallel(biogan.gen, list(range(torch.cuda.device_count())))\n",
    "#     biogan.info(f'Using {torch.cuda.device_count()} GPUs for PGPG Generator (via torch.nn.DataParallel)')\n",
    "# #   - load dataloader state (from model checkpoint)\n",
    "# if 'dataloader' in biogan.other_state_dicts.keys():\n",
    "#     dataloader.set_state(biogan.other_state_dicts['dataloader'])\n",
    "#     biogan.logger.debug(f'Loaded dataloader state! Current pem_index={dataloader.get_state()[\"perm_index\"]}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "EkVdzzTXr6Kj"
   },
   "source": [
    "### BioGAN Main Metrics Update call\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "eAltCxbUr6Kk"
   },
   "source": [
    "import json\n",
    "\n",
    "md = biogan.update_all_metrics(which='c2st')\n",
    "print(json.dumps(md, indent=4))\n",
    "biogan.visualize_metrics(upload=True, preview=True)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}