{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "train_1class.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Mount drive, unzip data, clone repo, install packages"
      ],
      "metadata": {
        "collapsed": false,
        "id": "Delm8c4mHpX_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "UmWJHQh7r6Kb"
      },
      "source": [
        "## 1.1) Mount Drive and define paths\n",
        "Run provided colab code to mount Google Drive. Then define dataset paths relative to mount point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1j__4kcZr6Kc"
      },
      "source": [
        "# noinspection PyUnresolvedReferences,PyPackageRequirements\n",
        "from google.colab import drive\n",
        "\n",
        "mount_root_abs = '/content/drive'\n",
        "drive.mount(mount_root_abs, force_remount=True)\n",
        "drive_root = f'{mount_root_abs}/MyDrive/ProjectGStorage'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2) Unzip Img directory in Colab\n",
        "By unzipping the `lin-48x80.zip` in Colab before running our model we gain significant disk reading speedups.\n",
        "So, the first step is to unzip images directory, and then save the image directory before proceeding."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "opylFk3cHpYC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "c7bh7lsyr6Kd"
      },
      "source": [
        "import os\n",
        "\n",
        "# Check if LIN Dataset is present / Download Dataset\n",
        "df_root_drive = f'{drive_root}/Datasets/LIN_48x80'\n",
        "if not os.path.exists(f'{df_root_drive}/lin-48x80.zip'):\n",
        "    !pip install kaggle --upgrade\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = drive_root\n",
        "    !mkdir -p $df_root_drive\n",
        "    !kaggle datasets download \"achariso/lin-48x80\" -p \"$df_root_drive\"\n",
        "\n",
        "# Unzip\n",
        "if not os.path.exists(f\"/content/data/LIN_48x80/LIN_Normalized_WT_size-48-80_train\"):\n",
        "  !pip install unzip\n",
        "  !mkdir -p \"/content/data/LIN_48x80\"\n",
        "  !cp -f \"$df_root_drive/lin-48x80.zip\" \"/content/data/LIN_48x80\"\n",
        "  !unzip -qq \"/content/data/LIN_48x80/lin-48x80.zip\" -d \"/content/data/LIN_48x80\"\n",
        "  if os.path.exists(f'/content/data/LIN_48x80/LIN_Normalized_WT_size-48-80_train'):\n",
        "      # Create symbolic links\n",
        "      !ln -s \"/content/data/LIN_48x80/LIN_Normalized_WT_size-48-80_train\" \"$df_root_drive/LIN_Normalized_WT_size-48-80_train\"\n",
        "      !ln -s \"/content/data/LIN_48x80/LIN_Normalized_WT_size-48-80_test\" \"$df_root_drive/LIN_Normalized_WT_size-48-80_test\"\n",
        "  else:\n",
        "      print(f'Error: Dataset not found at \"/content/data/LIN_48x80\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "rg1ABU-Dr6Ke"
      },
      "source": [
        "## 1.3) Clone GitHub repo\n",
        "Clone achariso/gans-thesis repo into /content/code\n",
        " using git clone.\n",
        " For more info see: https://medium.com/@purba0101/how-to-clone-private-github-repo-in-google-colab-using-ssh-77384cfef18f"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JleSSDBur6Ke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e923628-0d03-450f-dc97-7c344116ab00"
      },
      "source": [
        "repo_root = '/content/code/biogans'\n",
        "!rm -rf \"$repo_root\"\n",
        "if not os.path.exists(repo_root) and not os.path.exists(f'{repo_root}/requirements.txt'):\n",
        "    # Check that ssh keys exist\n",
        "    assert os.path.exists(f'{drive_root}/GitHub Keys')\n",
        "    id_rsa_abs_drive = f'{drive_root}/GitHub Keys/id_rsa'\n",
        "    id_rsa_pub_abs_drive = f'{id_rsa_abs_drive}.pub'\n",
        "    assert os.path.exists(id_rsa_abs_drive)\n",
        "    assert os.path.exists(id_rsa_pub_abs_drive)\n",
        "    # On first run: Add ssh key in repo\n",
        "    if not os.path.exists('/root/.ssh'):\n",
        "        # Transfer config file\n",
        "        ssh_config_abs_drive = f'{drive_root}/GitHub Keys/config'\n",
        "        assert os.path.exists(ssh_config_abs_drive)\n",
        "        !mkdir -p ~/.ssh\n",
        "        !cp -f \"$ssh_config_abs_drive\" ~/.ssh/\n",
        "        # # Add github.com to known hosts\n",
        "        !ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\n",
        "        # Test: !ssh -T git@github.com\n",
        "\n",
        "    # Remove any previous attempts\n",
        "    !rm -rf \"$repo_root\"\n",
        "    !mkdir -p \"$repo_root\"\n",
        "    # Clone repo\n",
        "    !git clone git@github.com:kth-ml-course-projects/biogans.git \"$repo_root\"\n",
        "    src_root = f'{repo_root}/src'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 886, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/146)\u001b[K\rremote: Counting objects:   1% (2/146)\u001b[K\rremote: Counting objects:   2% (3/146)\u001b[K\rremote: Counting objects:   3% (5/146)\u001b[K\rremote: Counting objects:   4% (6/146)\u001b[K\rremote: Counting objects:   5% (8/146)\u001b[K\rremote: Counting objects:   6% (9/146)\u001b[K\rremote: Counting objects:   7% (11/146)\u001b[K\rremote: Counting objects:   8% (12/146)\u001b[K\rremote: Counting objects:   9% (14/146)\u001b[K\rremote: Counting objects:  10% (15/146)\u001b[K\rremote: Counting objects:  11% (17/146)\u001b[K\rremote: Counting objects:  12% (18/146)\u001b[K\rremote: Counting objects:  13% (19/146)\u001b[K\rremote: Counting objects:  14% (21/146)\u001b[K\rremote: Counting objects:  15% (22/146)\u001b[K\rremote: Counting objects:  16% (24/146)\u001b[K\rremote: Counting objects:  17% (25/146)\u001b[K\rremote: Counting objects:  18% (27/146)\u001b[K\rremote: Counting objects:  19% (28/146)\u001b[K\rremote: Counting objects:  20% (30/146)\u001b[K\rremote: Counting objects:  21% (31/146)\u001b[K\rremote: Counting objects:  22% (33/146)\u001b[K\rremote: Counting objects:  23% (34/146)\u001b[K\rremote: Counting objects:  24% (36/146)\u001b[K\rremote: Counting objects:  25% (37/146)\u001b[K\rremote: Counting objects:  26% (38/146)\u001b[K\rremote: Counting objects:  27% (40/146)\u001b[K\rremote: Counting objects:  28% (41/146)\u001b[K\rremote: Counting objects:  29% (43/146)\u001b[K\rremote: Counting objects:  30% (44/146)\u001b[K\rremote: Counting objects:  31% (46/146)\u001b[K\rremote: Counting objects:  32% (47/146)\u001b[K\rremote: Counting objects:  33% (49/146)\u001b[K\rremote: Counting objects:  34% (50/146)\u001b[K\rremote: Counting objects:  35% (52/146)\u001b[K\rremote: Counting objects:  36% (53/146)\u001b[K\rremote: Counting objects:  37% (55/146)\u001b[K\rremote: Counting objects:  38% (56/146)\u001b[K\rremote: Counting objects:  39% (57/146)\u001b[K\rremote: Counting objects:  40% (59/146)\u001b[K\rremote: Counting objects:  41% (60/146)\u001b[K\rremote: Counting objects:  42% (62/146)\u001b[K\rremote: Counting objects:  43% (63/146)\u001b[K\rremote: Counting objects:  44% (65/146)\u001b[K\rremote: Counting objects:  45% (66/146)\u001b[K\rremote: Counting objects:  46% (68/146)\u001b[K\rremote: Counting objects:  47% (69/146)\u001b[K\rremote: Counting objects:  48% (71/146)\u001b[K\rremote: Counting objects:  49% (72/146)\u001b[K\rremote: Counting objects:  50% (73/146)\u001b[K\rremote: Counting objects:  51% (75/146)\u001b[K\rremote: Counting objects:  52% (76/146)\u001b[K\rremote: Counting objects:  53% (78/146)\u001b[K\rremote: Counting objects:  54% (79/146)\u001b[K\rremote: Counting objects:  55% (81/146)\u001b[K\rremote: Counting objects:  56% (82/146)\u001b[K\rremote: Counting objects:  57% (84/146)\u001b[K\rremote: Counting objects:  58% (85/146)\u001b[K\rremote: Counting objects:  59% (87/146)\u001b[K\rremote: Counting objects:  60% (88/146)\u001b[K\rremote: Counting objects:  61% (90/146)\u001b[K\rremote: Counting objects:  62% (91/146)\u001b[K\rremote: Counting objects:  63% (92/146)\u001b[K\rremote: Counting objects:  64% (94/146)\u001b[K\rremote: Counting objects:  65% (95/146)\u001b[K\rremote: Counting objects:  66% (97/146)\u001b[K\rremote: Counting objects:  67% (98/146)\u001b[K\rremote: Counting objects:  68% (100/146)\u001b[K\rremote: Counting objects:  69% (101/146)\u001b[K\rremote: Counting objects:  70% (103/146)\u001b[K\rremote: Counting objects:  71% (104/146)\u001b[K\rremote: Counting objects:  72% (106/146)\u001b[K\rremote: Counting objects:  73% (107/146)\u001b[K\rremote: Counting objects:  74% (109/146)\u001b[K\rremote: Counting objects:  75% (110/146)\u001b[K\rremote: Counting objects:  76% (111/146)\u001b[K\rremote: Counting objects:  77% (113/146)\u001b[K\rremote: Counting objects:  78% (114/146)\u001b[K\rremote: Counting objects:  79% (116/146)\u001b[K\rremote: Counting objects:  80% (117/146)\u001b[K\rremote: Counting objects:  81% (119/146)\u001b[K\rremote: Counting objects:  82% (120/146)\u001b[K\rremote: Counting objects:  83% (122/146)\u001b[K\rremote: Counting objects:  84% (123/146)\u001b[K\rremote: Counting objects:  85% (125/146)\u001b[K\rremote: Counting objects:  86% (126/146)\u001b[K\rremote: Counting objects:  87% (128/146)\u001b[K\rremote: Counting objects:  88% (129/146)\u001b[K\rremote: Counting objects:  89% (130/146)\u001b[K\rremote: Counting objects:  90% (132/146)\u001b[K\rremote: Counting objects:  91% (133/146)\u001b[K\rremote: Counting objects:  92% (135/146)\u001b[K\rremote: Counting objects:  93% (136/146)\u001b[K\rremote: Counting objects:  94% (138/146)\u001b[K\rremote: Counting objects:  95% (139/146)\u001b[K\rremote: Counting objects:  96% (141/146)\u001b[K\rremote: Counting objects:  97% (142/146)\u001b[K\rremote: Counting objects:  98% (144/146)\u001b[K\rremote: Counting objects:  99% (145/146)\u001b[K\rremote: Counting objects: 100% (146/146)\u001b[K\rremote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
            "remote: Total 886 (delta 84), reused 99 (delta 45), pack-reused 740\u001b[K\n",
            "Receiving objects: 100% (886/886), 1.67 MiB | 492.00 KiB/s, done.\n",
            "Resolving deltas: 100% (550/550), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ZI4gCyMdr6Kf"
      },
      "source": [
        "## 1.4) Install pip packages\n",
        "All required files are stored in a requirements.txt files at the repository's root.\n",
        "Use `pip install -r requirements.txt` from inside the dir to install required packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-v3aIKDXr6Kf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfcb7dc2-ff0e-4d46-8461-b6dda17e8192"
      },
      "source": [
        "%cd \"$repo_root\"\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/code/biogans\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.12.0+cu113)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (6.6.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (7.1.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (5.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (1.4.1)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (0.17.4)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (4.1.3)\n",
            "Requirement already satisfied: pydrive in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 17)) (1.3.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 20)) (3.1.0)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (3.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.10.0->-r requirements.txt (line 3)) (4.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (3.0.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->-r requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from prettytable->-r requirements.txt (line 8)) (4.11.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable->-r requirements.txt (line 8)) (0.2.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 9)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 9)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 9)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 9)) (2.10)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 13)) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 13)) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 13)) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 13)) (57.4.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 13)) (5.1.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 13)) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 13)) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 13)) (4.8.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client->-r requirements.txt (line 16)) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client->-r requirements.txt (line 16)) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client->-r requirements.txt (line 16)) (0.2.8)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.7/dist-packages (from pydrive->-r requirements.txt (line 17)) (1.12.11)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->pydrive->-r requirements.txt (line 17)) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->pydrive->-r requirements.txt (line 17)) (1.31.5)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->pydrive->-r requirements.txt (line 17)) (1.35.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->pydrive->-r requirements.txt (line 17)) (0.0.4)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->pydrive->-r requirements.txt (line 17)) (3.17.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->pydrive->-r requirements.txt (line 17)) (1.56.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->pydrive->-r requirements.txt (line 17)) (2022.1)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->pydrive->-r requirements.txt (line 17)) (21.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.2->pydrive->-r requirements.txt (line 17)) (4.2.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 18)) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 18)) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 18)) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 18)) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 18)) (1.44.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 18)) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 18)) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 18)) (1.8.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 18)) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable->-r requirements.txt (line 8)) (3.8.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 18)) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "oAwLF973r6Kg"
      },
      "source": [
        "# import os\n",
        "# os.kill(os.getpid(), 9)\n",
        "\n",
        "import torch\n",
        "assert torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "tKxMM8dTr6Kh"
      },
      "source": [
        "## 1.5) Add code/, */src/ to path\n",
        "This is necessary in order to be able to run the modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "SatLo0V5r6Kh"
      },
      "source": [
        "content_root_abs = f'{repo_root}'\n",
        "src_root_abs = f'{repo_root}/src'\n",
        "%env PYTHONPATH=\"/env/python:$content_root_abs:$src_root_abs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "tDfCUlQsr6Kh"
      },
      "source": [
        "# 2) Train BioGAN model on LIN Dataset\n",
        "In this section we run the actual training loop for BioGAN network. BioGAN consists of one and multi-channel DCGAN-like\n",
        "Generators and Discriminators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "T3i1qXear6Ki"
      },
      "source": [
        "### Actual Run\n",
        "Eventually, run the code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "MzfIhC7sr6Ki"
      },
      "source": [
        "chkpt_step = 'latest'   # supported: 'latest', <int>, None\n",
        "log_level = 'debug'     # supported: 'debug', 'info', 'warning', 'error', 'critical', 'fatal'\n",
        "device = 'cuda'             # supported: 'cpu', 'cuda', 'cuda:<GPU_INDEX>'\n",
        "gdrive_which = 'personal'   # supported: 'personal', 'auth'\n",
        "\n",
        "classes = 'Alp14'\n",
        "\n",
        "# Running with -i enables us to get variables defined inside the script (the script runs inline)\n",
        "%run -i src/train_setup.py --log_level $log_level --chkpt_step $chkpt_step --seed 42 --device $device --gdrive_which $gdrive_which -use_refresh_token --which_classes $classes\n",
        "%cd src/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "fo8tX5Omr6Ki"
      },
      "source": [
        "### BioGAN Training\n",
        "The code that follows defines the dataloaders/evaluators/models and the main training loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0xSTSSqOr6Kj"
      },
      "source": [
        "import torch\n",
        "from IPython.core.display import display\n",
        "from torch import Tensor\n",
        "from torch.nn import DataParallel\n",
        "# noinspection PyProtectedMember\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from datasets.lin import LINDataloader\n",
        "from modules.biogan_ind import BioGanInd1class\n",
        "from utils.metrics import GanEvaluator\n",
        "\n",
        "###################################\n",
        "###  Hyper-parameters settings  ###\n",
        "###################################\n",
        "#   - training\n",
        "n_epochs = 3175\n",
        "\n",
        "batch_size = 48 if not run_locally else 48\n",
        "train_test_splits = [90, 10]  # for a 90% training - 10% evaluation set split\n",
        "#   - evaluation\n",
        "metrics_n_samples = 1000 if not run_locally else 2\n",
        "metrics_batch_size = 32 if not run_locally else 1\n",
        "f1_k = 3 if not run_locally else 1\n",
        "#   - visualizations / checkpoints steps\n",
        "display_step = 300\n",
        "checkpoint_step = 600\n",
        "metrics_step = 1800  # evaluate model every 3 checkpoints\n",
        "\n",
        "###################################\n",
        "###   Dataset Initialization    ###\n",
        "###################################\n",
        "#   - the dataloader used to access the training dataset of cross-scale/pose image pairs at every epoch\n",
        "#     > len(dataloader) = <number of batches>\n",
        "#     > len(dataloader.dataset) = <number of total dataset items>\n",
        "dataloader = LINDataloader(dataset_fs_folder_or_root=datasets_groot, train_not_test=True,\n",
        "                           batch_size=batch_size, pin_memory=not run_locally, shuffle=True,\n",
        "                           which_classes=args.which_classes)\n",
        "dataset = dataloader.dataset\n",
        "#   - apply rudimentary tests\n",
        "assert issubclass(dataloader.__class__, DataLoader)\n",
        "assert len(dataloader) == len(dataset) // batch_size + (1 if len(dataset) % batch_size else 0)\n",
        "_x = next(iter(dataloader))\n",
        "assert tuple(_x.shape) == (batch_size, 2, 48, 80)\n",
        "\n",
        "###################################\n",
        "###    Models Initialization    ###\n",
        "###################################\n",
        "#   - initialize evaluator instance (used to run GAN evaluation metrics: FID, IS, PRECISION, RECALL, F1 and SSIM)\n",
        "evaluator = GanEvaluator(model_fs_folder_or_root=models_groot, gen_dataset=dataset, device=exec_device,\n",
        "                         z_dim=-1, n_samples=metrics_n_samples, batch_size=metrics_batch_size, f1_k=f1_k,\n",
        "                         ssim_c_img=2)\n",
        "#   - initialize model\n",
        "chkpt_step = args.chkpt_step\n",
        "try:\n",
        "    if chkpt_step == 'latest':\n",
        "        _chkpt_step = chkpt_step\n",
        "    elif isinstance(chkpt_step, str) and chkpt_step.isdigit():\n",
        "        _chkpt_step = int(chkpt_step)\n",
        "    else:\n",
        "        _chkpt_step = None\n",
        "except NameError:\n",
        "    _chkpt_step = None\n",
        "BioGanInd1class.PROTEIN_CLASS = classes\n",
        "biogan = BioGanInd1class(model_fs_folder_or_root=models_groot, config_id='default', dataset_len=len(dataset),\n",
        "                         chkpt_epoch=_chkpt_step, evaluator=evaluator, device=exec_device, log_level=log_level,\n",
        "                         gen_transforms=dataloader.transforms)\n",
        "biogan.logger.debug(f'Using device: {str(exec_device)}')\n",
        "biogan.logger.debug(f'Model initialized. Number of params = {biogan.nparams_hr}')\n",
        "# FIX: Warmup counters before first batch\n",
        "if biogan.step is None:\n",
        "    biogan.gforward(batch_size=batch_size)\n",
        "    biogan.logger.debug(f'Model warmed-up (internal counters).')\n",
        "# #   - setup multi-GPU training\n",
        "# if torch.cuda.device_count() > 1:\n",
        "#     biogan.gen = DataParallel(biogan.gen, list(range(torch.cuda.device_count())))\n",
        "#     biogan.info(f'Using {torch.cuda.device_count()} GPUs for PGPG Generator (via torch.nn.DataParallel)')\n",
        "# #   - load dataloader state (from model checkpoint)\n",
        "# if 'dataloader' in biogan.other_state_dicts.keys():\n",
        "#     dataloader.set_state(biogan.other_state_dicts['dataloader'])\n",
        "#     biogan.logger.debug(f'Loaded dataloader state! Current pem_index={dataloader.get_state()[\"perm_index\"]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "EkVdzzTXr6Kj"
      },
      "source": [
        "### BioGAN Main training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "eAltCxbUr6Kk"
      },
      "source": [
        "from tqdm.autonotebook import tqdm\n",
        "from utils.dep_free import in_notebook\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "###################################\n",
        "###       Training Loop         ###\n",
        "###################################\n",
        "#   - start training loop from last checkpoint's epoch and step\n",
        "torch.cuda.empty_cache()\n",
        "gcapture_ready = True\n",
        "async_results = None\n",
        "biogan.logger.info(f'[training loop] STARTING (epoch={biogan.epoch}, step={biogan.initial_step})')\n",
        "biogan.initial_step += 1\n",
        "for epoch in range(biogan.epoch, n_epochs):\n",
        "    image_1: Tensor\n",
        "    image_2: Tensor\n",
        "    pose_2: Tensor\n",
        "\n",
        "    # noinspection PyProtectedMember\n",
        "    d = {\n",
        "        'step': biogan.step,\n",
        "        'initial_step': biogan.initial_step,\n",
        "        'epoch': biogan.epoch,\n",
        "        '_counter': biogan._counter,\n",
        "        'epoch_inc': biogan.epoch_inc,\n",
        "    }\n",
        "    # initial_step = biogan.initial_step % len(dataloader)\n",
        "    biogan.logger.debug('[START OF EPOCH] ' + str(d))\n",
        "    for x in tqdm(dataloader):\n",
        "        # Transfer image batches to GPU\n",
        "        x = x.to(exec_device)\n",
        "\n",
        "        # Perform a forward + backward pass + weight update on the Generator & Discriminator models\n",
        "        disc_loss, gen_loss = biogan(x)\n",
        "\n",
        "        # Metrics & Checkpoint Code\n",
        "        if biogan.step % checkpoint_step == 0:\n",
        "            # Check if another upload is pending\n",
        "            if not gcapture_ready and async_results:\n",
        "                # Wait for previous upload to finish\n",
        "                biogan.logger.warning('Waiting for previous gcapture() to finish...')\n",
        "                [r.wait() for r in async_results]\n",
        "                biogan.logger.warning('DONE! Starting new capture now.')\n",
        "            # Capture current model state, including metrics and visualizations\n",
        "            async_results = biogan.gcapture(checkpoint=True, metrics=biogan.step % metrics_step == 0, visualizations=True,\n",
        "                                            dataloader=dataloader, in_parallel=True, show_progress=True,\n",
        "                                            delete_after=False)\n",
        "        # Visualization code\n",
        "        elif biogan.step % display_step == 0:\n",
        "            visualization_img = biogan.visualize()\n",
        "            visualization_img.show() if not in_notebook() else display(visualization_img)\n",
        "\n",
        "        # Check if a pending checkpoint upload has finished\n",
        "        if async_results:\n",
        "            gcapture_ready = all([r.ready() for r in async_results])\n",
        "            if gcapture_ready:\n",
        "                biogan.logger.info(f'gcapture() finished')\n",
        "                if biogan.latest_checkpoint_had_metrics:\n",
        "                    biogan.logger.info(str(biogan.latest_metrics))\n",
        "                async_results = None\n",
        "\n",
        "        # If run locally one pass is enough\n",
        "        if run_locally and gcapture_ready:\n",
        "            break\n",
        "\n",
        "    # If run locally one pass is enough\n",
        "    if run_locally:\n",
        "        break\n",
        "\n",
        "    # noinspection PyProtectedMember\n",
        "    d = {\n",
        "        'step': biogan.step,\n",
        "        'initial_step': biogan.initial_step,\n",
        "        'epoch': biogan.epoch,\n",
        "        '_counter': biogan._counter,\n",
        "        'epoch_inc': biogan.epoch_inc,\n",
        "    }\n",
        "    biogan.logger.debug('[END OF EPOCH] ' + str(d))\n",
        "\n",
        "# Check if a pending checkpoint exists\n",
        "if async_results:\n",
        "    ([r.wait() for r in async_results])\n",
        "    biogan.logger.info(f'last gcapture() finished')\n",
        "    if biogan.latest_checkpoint_had_metrics:\n",
        "        biogan.logger.info(str(biogan.latest_metrics))\n",
        "    async_results = None\n",
        "\n",
        "# Training finished!\n",
        "biogan.logger.info('[training loop] DONE')\n",
        "biogan.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "4eo2nbawr6Kl"
      },
      "source": [
        "# 3) Evaluate PGPG\n",
        "In this section we evaluate the generation performance of our trained network using the SOTA GAN evaluation metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "PuHwKZpAr6Kl"
      },
      "source": [
        "## 3.1) Get the metrics evolution plots\n",
        "We plot how the metrics evolved during training. The GAN is **not** trained to minimize those metrics (they are\n",
        "calculated using `torch.no_grad()`) and thus this evolution merely depends on the network and showcases the correlation\n",
        "between the GAN evaluation metrics, and the losses (e.g. adversarial & reconstruction) used to optimize the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1RZWIdiEr6Kl"
      },
      "source": [
        "# Since the PGPG implements utils.ifaces.Visualizable, we can\n",
        "# directly call visualize_metrics() on the model instance.\n",
        "_ = biogan.visualize_metrics(upload=True, preview=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "0JS-838er6Kl"
      },
      "source": [
        "## 3.2) Evaluate Generated Samples\n",
        "In order to evaluate generated samples and compare model with other GAN architectures trained on the same dataset.\n",
        "For this purpose we will re-calculate the evaluation metrics as stated above, but with a much bigger number of samples.\n",
        "In this way, the metrics will be more trustworthy and comparable with the corresponding metrics in the original paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "TtWHtcgsr6Km"
      },
      "source": [
        "# Initialize a new evaluator instance\n",
        "# (used to run GAN evaluation metrics: FID, IS, PRECISION, RECALL, F1 and SSIM)\n",
        "evaluator = GanEvaluator(model_fs_folder_or_root=models_groot, gen_dataset=dataloader.test_set, target_index=0,\n",
        "                         device=exec_device, n_samples=10000, batch_size=64, f1_k=f1_k, ssim_c_img=2)\n",
        "# Run the evaluator\n",
        "metrics_dict = evaluator.evaluate(gen=biogan.gen, metric_name='all', show_progress=True)\n",
        "\n",
        "# Print results\n",
        "import json\n",
        "print(json.dumps(metrics_dict, indent=4))\n",
        "\n",
        "#\n",
        "#-----------\n",
        "# Epoch 93\n",
        "#----------\n",
        "# \n",
        "# Training Set:\n",
        "# {\n",
        "#   \"fid\": 16.195581436157227\n",
        "#   \"is\": 3.82967472076416\n",
        "#   \"f1\": 0.8827780485153198\n",
        "#   \"precision\": 0.8856828808784485\n",
        "#   \"recall\": 0.8798921704292297\n",
        "#   \"ssim\": 0.8029271364212036\n",
        "# }\n",
        "#\n",
        "# Test Set:\n",
        "# {\n",
        "#     \"fid\": 26.503515243530273,\n",
        "#     \"is\": 2.957645606994629,\n",
        "#     \"f1\": 0.8494825959205627,\n",
        "#     \"precision\": 0.8351463675498962,\n",
        "#     \"recall\": 0.8643196225166321,\n",
        "#     \"ssim\": 0.7690791009871171\n",
        "# }\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}