{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Mount drive, unzip data, clone repo, install packages\n",
    "\n",
    "## 1.1) Define paths\n",
    "Google Drive root: `/kaggle/working/GoogleDrive`\n",
    "Dataset paths are relative to mount point (`/kaggle/input` & `/kaggle/working`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-22T20:03:25.666632Z",
     "iopub.status.busy": "2022-03-22T20:03:25.665882Z",
     "iopub.status.idle": "2022-03-22T20:03:29.754744Z",
     "shell.execute_reply": "2022-03-22T20:03:29.753465Z",
     "shell.execute_reply.started": "2022-03-22T20:03:25.666534Z"
    },
    "id": "UmWJHQh7r6Kb",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create root directory if not exists\n",
    "drive_root = '/kaggle/working/GoogleDrive'\n",
    "!mkdir -p \"$drive_root\"\n",
    "\n",
    "# Define Google Drive related paths\n",
    "!mkdir -p \"$drive_root\"\n",
    "!mkdir -p \"$drive_root/Models\"\n",
    "!mkdir -p \"$drive_root/Datasets\"\n",
    "!mkdir -p \"$drive_root/GitHub Keys\"\n",
    "\n",
    "# Define asset paths\n",
    "git_keys_root = '/kaggle/input/gitkeys-biogans'\n",
    "assert os.path.exists(git_keys_root), f'git_keys_root={git_keys_root}: NOT FOUND'\n",
    "client_secrets_path = f'{git_keys_root}/client_secrets.json'\n",
    "assert os.path.exists(client_secrets_path), f'client_secrets_path={client_secrets_path}: NOT FOUND'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2) Link inputs available via Kaggle Datasets to corresponding GoogleDrive paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-22T20:03:29.763158Z",
     "iopub.status.busy": "2022-03-22T20:03:29.762440Z",
     "iopub.status.idle": "2022-03-22T20:03:30.708399Z",
     "shell.execute_reply": "2022-03-22T20:03:30.707188Z",
     "shell.execute_reply.started": "2022-03-22T20:03:29.763111Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Copy the fonts' dir inside local Google Drive root\n",
    "!cp -rf \"/kaggle/input/mplfonts/Fonts\" \"$drive_root\"\n",
    "if not os.path.exists(f'{drive_root}/client_secrets.json'):\n",
    "    !cp \"$client_secrets_path\" \"$drive_root/\"\n",
    "\n",
    "# Link the Inceptionv3 & VGG models checkpoints inside local Google Drive root\n",
    "if not os.path.exists(f'{drive_root}/Models/model_name=inceptionv3'):\n",
    "    !cp -rfa \"/kaggle/input/inception-model/.\" \"$drive_root/Models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-22T20:03:30.712220Z",
     "iopub.status.busy": "2022-03-22T20:03:30.711856Z",
     "iopub.status.idle": "2022-03-22T20:03:31.475270Z",
     "shell.execute_reply": "2022-03-22T20:03:31.474073Z",
     "shell.execute_reply.started": "2022-03-22T20:03:30.712172Z"
    },
    "id": "c7bh7lsyr6Kd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Link Dataset\n",
    "if not os.path.exists(f'{drive_root}/Datasets/LIN_48x80/LIN_Normalized_WT_size-48-80_train'):\n",
    "    !mkdir -p \"$drive_root/Datasets/LIN_48x80\"\n",
    "    !ln -s \"/kaggle/input/lin-48x80/LIN_Normalized_WT_size-48-80_train\" \"$drive_root/Datasets/LIN_48x80/\"\n",
    "    !ln -s \"/kaggle/input/lin-48x80/LIN_Normalized_WT_size-48-80_test\" \"$drive_root/Datasets/LIN_48x80/\"\n",
    "!ls \"$drive_root/Datasets/LIN_48x80\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rg1ABU-Dr6Ke"
   },
   "source": [
    "## 1.3) Clone GitHub repo\n",
    "Clone achariso/gans-thesis repo into/content/code using git clone.\n",
    "For more info see: https://medium.com/@purba0101/how-to-clone-private-github-repo-in-google-colab-using-ssh-77384cfef18f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-22T20:03:31.480234Z",
     "iopub.status.busy": "2022-03-22T20:03:31.479529Z",
     "iopub.status.idle": "2022-03-22T20:03:36.420667Z",
     "shell.execute_reply": "2022-03-22T20:03:36.419406Z",
     "shell.execute_reply.started": "2022-03-22T20:03:31.480167Z"
    },
    "id": "JleSSDBur6Ke",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "repo_root = '/content/code/biogans'\n",
    "!rm -rf \"$repo_root\"\n",
    "if not os.path.exists(repo_root):\n",
    "    # Check that ssh keys exist\n",
    "    assert os.path.exists(f'{git_keys_root}')\n",
    "    id_rsa_abs_drive = f'{git_keys_root}/id_rsa'\n",
    "    id_rsa_pub_abs_drive = f'{id_rsa_abs_drive}.pub'\n",
    "    assert os.path.exists(id_rsa_abs_drive)\n",
    "    assert os.path.exists(id_rsa_pub_abs_drive)\n",
    "    # On first run: Add ssh key in repo\n",
    "    if not os.path.exists('/root/.ssh'):\n",
    "        # Transfer config file\n",
    "        ssh_config_abs_drive = f'{git_keys_root}/config'\n",
    "        assert os.path.exists(ssh_config_abs_drive)\n",
    "        !mkdir -p ~/.ssh\n",
    "        !cp -f \"$ssh_config_abs_drive\" ~/.ssh/\n",
    "        # # Add GitHub.com to known hosts\n",
    "        !ssh-keyscan -t rsa github.com >> ~/.ssh/ known_hosts\n",
    "        # Test: !ssh -T git@github.com\n",
    "\n",
    "    # Remove any previous attempts\n",
    "    !rm -rf \"$repo_root\"\n",
    "    !mkdir -p \"$repo_root\"\n",
    "    # Clone repo\n",
    "    !git clone git@github.com:kth-ml-course-projects/biogans.git \"$repo_root\"\n",
    "    src_root = f'{repo_root}/src'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZI4gCyMdr6Kf"
   },
   "source": [
    "## 1.4) Install pip packages\n",
    "All required files are stored in a requirements.txt files at the repository's root.\n",
    "Use `pip install -r requirements.txt` from inside the dir to install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-22T20:03:36.424874Z",
     "iopub.status.busy": "2022-03-22T20:03:36.424588Z",
     "iopub.status.idle": "2022-03-22T20:03:46.927417Z",
     "shell.execute_reply": "2022-03-22T20:03:46.926225Z",
     "shell.execute_reply.started": "2022-03-22T20:03:36.424844Z"
    },
    "id": "-v3aIKDXr6Kf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "% cd \"$repo_root\"\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-22T20:03:46.931991Z",
     "iopub.status.busy": "2022-03-22T20:03:46.931692Z",
     "iopub.status.idle": "2022-03-22T20:03:47.617971Z",
     "shell.execute_reply": "2022-03-22T20:03:47.616873Z",
     "shell.execute_reply.started": "2022-03-22T20:03:46.931958Z"
    },
    "id": "oAwLF973r6Kg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.kill(os.getpid(), 9)\n",
    "\n",
    "import torch\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKxMM8dTr6Kh"
   },
   "source": [
    "## 1.5) Add code/, */src/ to path\n",
    "This is necessary in order to be able to run the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-22T20:03:47.621158Z",
     "iopub.status.busy": "2022-03-22T20:03:47.620559Z",
     "iopub.status.idle": "2022-03-22T20:03:47.629291Z",
     "shell.execute_reply": "2022-03-22T20:03:47.628197Z",
     "shell.execute_reply.started": "2022-03-22T20:03:47.621081Z"
    },
    "id": "SatLo0V5r6Kh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "content_root_abs = f'{repo_root}'\n",
    "src_root_abs = f'{repo_root}/src'\n",
    "% env PYTHONPATH=\"/env/python:$content_root_abs:$src_root_abs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDfCUlQsr6Kh"
   },
   "source": [
    "# 2) Train BioGAN model on LIN Dataset\n",
    "In this section we run the actual training loop for BioGAN network. BioGAN consists of one and multichannel DCGAN-like\n",
    "Generators and Discriminators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3i1qXear6Ki"
   },
   "source": [
    "### Actual Run\n",
    "Eventually, run the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-22T20:03:47.631550Z",
     "iopub.status.busy": "2022-03-22T20:03:47.630817Z",
     "iopub.status.idle": "2022-03-22T20:03:53.769631Z",
     "shell.execute_reply": "2022-03-22T20:03:53.768498Z",
     "shell.execute_reply.started": "2022-03-22T20:03:47.631472Z"
    },
    "id": "MzfIhC7sr6Ki",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chkpt_step = 'latest'  # supported: 'latest', <int>, None\n",
    "log_level = 'debug'  # supported: 'debug', 'info', 'warning', 'error', 'critical', 'fatal'\n",
    "device = 'cuda'  # supported: 'cpu', 'cuda', 'cuda:<GPU_INDEX>'\n",
    "gdrive_which = 'personal'  # supported: 'personal', 'auth'\n",
    "\n",
    "classes = 'Alp14'\n",
    "\n",
    "# Running with -i enables us to get variables defined inside the script (the script runs inline)\n",
    "% run -i \"src/train_setup.py\" --log_level $log_level --chkpt_step $chkpt_step --seed 42 --device $device --gdrive_which $gdrive_which -use_refresh_token --which_classes $classes\n",
    "% cd \"src/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fo8tX5Omr6Ki",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### BioGAN Training\n",
    "The code that follows defines the dataloaders/evaluators/models and the main training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-22T20:05:38.944389Z",
     "iopub.status.busy": "2022-03-22T20:05:38.944009Z",
     "iopub.status.idle": "2022-03-22T20:06:11.189400Z",
     "shell.execute_reply": "2022-03-22T20:06:11.188378Z",
     "shell.execute_reply.started": "2022-03-22T20:05:38.944356Z"
    },
    "id": "0xSTSSqOr6Kj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.core.display import display\n",
    "from torch import Tensor\n",
    "from torch.nn import DataParallel\n",
    "# noinspection PyProtectedMember\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets.lin import LINDataloader\n",
    "from modules.biogan import OneClassBioGan\n",
    "from utils.metrics import GanEvaluator\n",
    "\n",
    "###################################\n",
    "###  Hyper-parameters settings  ###\n",
    "###################################\n",
    "#   - training\n",
    "n_epochs = 3200\n",
    "\n",
    "batch_size = 64 if not run_locally else 48\n",
    "train_test_splits = [90, 10]  # for a 90% training - 10% evaluation set split\n",
    "#   - evaluation\n",
    "metrics_n_samples = 1000 if not run_locally else 2\n",
    "metrics_batch_size = 32 if not run_locally else 1\n",
    "f1_k = 3 if not run_locally else 1\n",
    "#   - visualizations/ checkpoints steps\n",
    "display_step = 300\n",
    "checkpoint_step = 600\n",
    "metrics_step = 1800  # evaluate model every 3 checkpoints\n",
    "\n",
    "###################################\n",
    "###   Dataset Initialization    ###\n",
    "###################################\n",
    "#   - the dataloader used to access the training dataset of cross-scale/pose image pairs at every epoch\n",
    "#     > len(dataloader) = <number of batches>\n",
    "#     > len(dataloader.dataset) = <number of total dataset items>\n",
    "# FIX: Add subfolders to GoogleDrive\n",
    "datasets_groot.subfolder_by_name('LIN_48x80').subfolder_by_name_or_create('LIN_Normalized_WT_size-48-80_train')\n",
    "datasets_groot.subfolder_by_name('LIN_48x80').subfolder_by_name_or_create('LIN_Normalized_WT_size-48-80_test')\n",
    "dataloader = LINDataloader(dataset_fs_folder_or_root=datasets_groot, train_not_test=True,\n",
    "                           batch_size=batch_size, pin_memory=not run_locally, shuffle=True,\n",
    "                           which_classes=args.which_classes)\n",
    "dataset = dataloader.dataset\n",
    "# noinspection PyProtectedMember\n",
    "dataset.logger.debug('Transforms: ' + repr(dataset._transforms))\n",
    "#   - apply rudimentary tests\n",
    "assert issubclass(dataloader.__class__, DataLoader)\n",
    "assert len(dataloader) == len(dataset) // batch_size + (1 if len(dataset) % batch_size else 0)\n",
    "_x = next(iter(dataloader))\n",
    "assert tuple(_x.shape) == (batch_size, 2, 48, 80)\n",
    "\n",
    "###################################\n",
    "###    Models Initialization    ###\n",
    "###################################\n",
    "#   - initialize evaluator instance (used to run GAN evaluation metrics: FID, IS, PRECISION, RECALL, F1 and SSIM)\n",
    "evaluator = GanEvaluator(model_fs_folder_or_root=models_groot, gen_dataset=dataset, device=exec_device,\n",
    "                         z_dim=-1, n_samples=metrics_n_samples, batch_size=metrics_batch_size, f1_k=f1_k,\n",
    "                         ssim_c_img=2)\n",
    "#   - initialize model\n",
    "chkpt_step = args.chkpt_step\n",
    "try:\n",
    "    if chkpt_step == 'latest':\n",
    "        _chkpt_step = chkpt_step\n",
    "    elif isinstance(chkpt_step, str) and chkpt_step.isdigit():\n",
    "        _chkpt_step = int(chkpt_step)\n",
    "    else:\n",
    "        _chkpt_step = None\n",
    "except NameError:\n",
    "    _chkpt_step = None\n",
    "OneClassBioGan.PROTEIN_CLASS = classes\n",
    "biogan = OneClassBioGan(model_fs_folder_or_root=models_groot, config_id='default', dataset_len=len(dataset),\n",
    "                        chkpt_epoch=_chkpt_step, evaluator=evaluator, device=exec_device, log_level=log_level,\n",
    "                        gen_transforms=dataloader.transforms)\n",
    "biogan.logger.debug(f'Using device: {str(exec_device)}')\n",
    "biogan.logger.debug(f'Model initialized. Number of params = {biogan.nparams_hr}')\n",
    "# FIX: Warmup counters before first batch\n",
    "if biogan.step is None:\n",
    "    biogan.gforward(batch_size=batch_size)\n",
    "    biogan.logger.debug(f'Model warmed-up (internal counters).')\n",
    "# #   - setup multi-GPU training\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     biogan.gen = DataParallel(biogan.gen, list(range(torch.cuda.device_count())))\n",
    "#     biogan.info(f'Using {torch.cuda.device_count()} GPUs for PGPG Generator (via torch.nn.DataParallel)')\n",
    "# #   - load dataloader state (from model checkpoint)\n",
    "# if 'dataloader' in biogan.other_state_dicts.keys():\n",
    "#     dataloader.set_state(biogan.other_state_dicts['dataloader'])\n",
    "#     biogan.logger.debug(f'Loaded dataloader state! Current pem_index={dataloader.get_state()[\"perm_index\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkVdzzTXr6Kj"
   },
   "source": [
    "### BioGAN Main training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-22T20:06:25.389304Z",
     "iopub.status.busy": "2022-03-22T20:06:25.388938Z"
    },
    "id": "eAltCxbUr6Kk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "from utils.dep_free import in_notebook\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "###################################\n",
    "###       Training Loop         ###\n",
    "###################################\n",
    "#   - start training loop from last checkpoint's epoch and step\n",
    "torch.cuda.empty_cache()\n",
    "gcapture_ready = True\n",
    "async_results = None\n",
    "biogan.logger.info(f'[training loop] STARTING (epoch={biogan.epoch}, step={biogan.initial_step})')\n",
    "biogan.initial_step += 1\n",
    "for epoch in range(biogan.epoch, n_epochs):\n",
    "    image_1: Tensor\n",
    "    image_2: Tensor\n",
    "    pose_2: Tensor\n",
    "\n",
    "    # noinspection PyProtectedMember\n",
    "    d = {\n",
    "        'step': biogan.step,\n",
    "        'initial_step': biogan.initial_step,\n",
    "        'epoch': biogan.epoch,\n",
    "        '_counter': biogan._counter,\n",
    "        'epoch_inc': biogan.epoch_inc,\n",
    "    }\n",
    "    # initial_step = biogan.initial_step % len(dataloader)\n",
    "    biogan.logger.debug('[START OF EPOCH] ' + str(d))\n",
    "    progress_bar = tqdm(dataloader)\n",
    "    progress_bar.set_description(f'[e {str(epoch).zfill(4)}/{str(n_epochs).zfill(4)}]' +\n",
    "                                 f'[g --.-- | d --.--]')\n",
    "    for x in progress_bar:\n",
    "        # Transfer image batches to GPU\n",
    "        x = x.to(exec_device)\n",
    "\n",
    "        # Perform a forward + backward pass + weight update on the Generator & Discriminator models\n",
    "        disc_loss, gen_loss = biogan(x)\n",
    "        progress_bar.set_description(f'[e {str(epoch).zfill(4)}/{str(n_epochs).zfill(4)}]' +\n",
    "                                     f'[g {round(gen_loss.item(), 2)} | d {round(disc_loss.item(), 2)}]')\n",
    "\n",
    "        # Metrics & Checkpoint Code\n",
    "        if biogan.step % checkpoint_step == 0:\n",
    "            # Check if another upload is pending\n",
    "            if not gcapture_ready and async_results:\n",
    "                # Wait for previous upload to finish\n",
    "                biogan.logger.warning('Waiting for previous gcapture() to finish...')\n",
    "                [r.wait() for r in async_results]\n",
    "                biogan.logger.warning('DONE! Starting new capture now.')\n",
    "            # Capture current model state, including metrics and visualizations\n",
    "            async_results = biogan.gcapture(checkpoint=True, metrics=biogan.step % metrics_step == 0,\n",
    "                                            visualizations=True,\n",
    "                                            dataloader=dataloader, in_parallel=True, show_progress=True,\n",
    "                                            delete_after=False)\n",
    "        # Visualization code\n",
    "        elif biogan.step % display_step == 0:\n",
    "            visualization_img = biogan.visualize()\n",
    "            visualization_img.show() if not in_notebook() else display(visualization_img)\n",
    "\n",
    "        # Check if a pending checkpoint upload has finished\n",
    "        if async_results:\n",
    "            gcapture_ready = all([r.ready() for r in async_results])\n",
    "            if gcapture_ready:\n",
    "                biogan.logger.info(f'gcapture() finished')\n",
    "                if biogan.latest_checkpoint_had_metrics:\n",
    "                    biogan.logger.info(str(biogan.latest_metrics))\n",
    "                async_results = None\n",
    "\n",
    "        # If run locally one pass is enough\n",
    "        if run_locally and gcapture_ready:\n",
    "            break\n",
    "\n",
    "    # If run locally one pass is enough\n",
    "    if run_locally:\n",
    "        break\n",
    "\n",
    "    # noinspection PyProtectedMember\n",
    "    d = {\n",
    "        'step': biogan.step,\n",
    "        'initial_step': biogan.initial_step,\n",
    "        'epoch': biogan.epoch,\n",
    "        '_counter': biogan._counter,\n",
    "        'epoch_inc': biogan.epoch_inc,\n",
    "    }\n",
    "    biogan.logger.debug('[END OF EPOCH] ' + str(d))\n",
    "\n",
    "# Check if a pending checkpoint exists\n",
    "if async_results:\n",
    "    ([r.wait() for r in async_results])\n",
    "    biogan.logger.info(f'last gcapture() finished')\n",
    "    if biogan.latest_checkpoint_had_metrics:\n",
    "        biogan.logger.info(str(biogan.latest_metrics))\n",
    "    async_results = None\n",
    "\n",
    "# Training finished!\n",
    "biogan.logger.info('[training loop] DONE')\n",
    "\n",
    "# Take last checkpoint\n",
    "biogan.gcapture(checkpoint=True, metrics=True, visualizations=True, in_parallel=False, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eo2nbawr6Kl"
   },
   "source": [
    "# 3) Evaluate PGPG\n",
    "In this section we evaluate the generation performance of our trained network using the SOTA GAN evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuHwKZpAr6Kl"
   },
   "source": [
    "## 3.1) Get the metrics evolution plots\n",
    "We plot how the metrics evolved during training. The GAN is **not** trained to minimize those metrics (they are\n",
    "calculated using `torch.no_grad()`) and thus this evolution merely depends on the network and showcases the correlation\n",
    "between the GAN evaluation metrics, and the losses (e.g. adversarial & reconstruction) used to optimize the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RZWIdiEr6Kl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Since the PGPG implements utils.ifaces.Visualizable, we can\n",
    "# directly call visualize_metrics() on the model instance.\n",
    "_ = biogan.visualize_metrics(upload=True, preview=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JS-838er6Kl"
   },
   "source": [
    "## 3.2) Evaluate Generated Samples\n",
    "In order to evaluate generated samples and compare model with other GAN architectures trained on the same dataset.\n",
    "For this purpose we will re-calculate the evaluation metrics as stated above, but with a much bigger number of samples.\n",
    "In this way, the metrics will be more trustworthy and comparable with the corresponding metrics in the original paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtWHtcgsr6Km",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a new evaluator instance\n",
    "# (used to run GAN evaluation metrics: FID, IS, PRECISION, RECALL, F1 and SSIM)\n",
    "dataloader = LINDataloader(dataset_fs_folder_or_root=datasets_groot, train_not_test=False,\n",
    "                           batch_size=batch_size, pin_memory=not run_locally, shuffle=True,\n",
    "                           which_classes=args.which_classes)\n",
    "dataset = dataloader.dataset\n",
    "evaluator = GanEvaluator(model_fs_folder_or_root=models_groot, gen_dataset=data_set, target_index=0,\n",
    "                         device=exec_device, n_samples=480, batch_size=64, f1_k=f1_k, ssim_c_img=2)\n",
    "# Run the evaluator\n",
    "metrics_dict = evaluator.evaluate(gen=biogan.gen, metric_name='all', show_progress=True)\n",
    "\n",
    "# Print results\n",
    "import json\n",
    "\n",
    "print(json.dumps(metrics_dict, indent=4))\n",
    "\n",
    "#\n",
    "#-------------\n",
    "# Epoch 3200\n",
    "#------------\n",
    "# \n",
    "# Training Set:\n",
    "# {\n",
    "#     \"fid\": 8.034801483154297,\n",
    "#     \"is\": 1.8340710401535034,\n",
    "#     \"f1\": 0.9129236936569214,\n",
    "#     \"precision\": 0.8901609182357788,\n",
    "#     \"recall\": 0.9368811845779419,\n",
    "#     \"ssim\": 0.6353044509887695,\n",
    "#     \"ppl\": 1.3186851513091041e-13\n",
    "# }\n",
    "#\n",
    "# Test Set:\n",
    "# {\n",
    "#     \"fid\": 15.846786499023438,\n",
    "#     \"is\": 1.8307020664215088,\n",
    "#     \"f1\": 0.8982593417167664,\n",
    "#     \"precision\": 0.8671875,\n",
    "#     \"recall\": 0.931640625,\n",
    "#     \"ssim\": 0.635291337966919,\n",
    "#     \"ppl\": 1.7720975511656412e-13\n",
    "# }\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}